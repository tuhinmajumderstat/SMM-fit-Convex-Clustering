library(seqinr)
library(cvxclustr)
library(gtools)
library(dplyr)
library(Matrix)
library(parallel)
find_index <- function(input,base)
{
m <- length(input)
d <- base
ind <- sum((input-1)* d^c(0:(m-1)))+1
ind
}
log_trunc <- function(x)
{
if(x>0)
{
return (log(x))
}else{
return (0)
}
}
fit_reference_model <- function(dat,d=4,m=3,knn=5,phi=100,lambda_seq=seq(0,100,by=0.1))
{
dat_num <- matrix(0,nrow=length(dat[[1]]),ncol=1)
for(i in 1:length(dat[[1]]))
{
if(dat[[1]][i]=="a")
{
dat_num[i] <- 1
}else{
if(dat[[1]][i]=="g")
{
dat_num[i] <- 2
}else{
if(dat[[1]][i]=="t")
{
dat_num[i] <- 3
}else{
dat_num[i] <- 4
}
}
}
}
chain <- dat_num
n <- length(chain)
p <- d^m
all_comb <- expand.grid(rep(list(1:d), m))
N <- matrix(0,p,d,byrow=TRUE)
for(i in (m+1):n)
{
prev <- chain[(i-m):(i-1)]
j <- chain[i]
tuple_ind <- find_index(prev,d)
N[tuple_ind,j] <- N[tuple_ind,j]+1
}
X <- N/rowSums(N)
X <- t(X)
keep_rows <- which(rowSums(N)>0)
X <- X[,keep_rows]
nu <- 1.8*(1/p)
wt <- matrix(0,nrow = ncol(X),ncol=ncol(X))
for(i in 1:ncol(X))
{
for(j in 1:ncol(X))
{
wt[i,j] <- max(abs(X[,i]-X[,j]))
}
}
w <- array()
start <- 1
for(i in 1:(ncol(X)-1))
{
for(j in (i+1):ncol(X))
{
w[start] <- exp(-phi*wt[i,j])
start <- start+1
}
}
w <- knn_weights(w,k=knn,p)
model_all <- cvxclust(X,w=w,gamma=lambda_seq,nu=nu,tol=1e-7)
N_data <- as.data.frame(N[keep_rows,])
names(N_data) <- paste0("state",c(1:d))
BIC <- matrix(0,length(lambda_seq),1,byrow=TRUE)
for(j in 1:length(lambda_seq))
{
A <- create_adjacency(model_all$V[[j]],w,ncol(X))
A <- as(A,"dgCMatrix")
assignment <- find_clusters(A)
cluster_id <- unique(assignment$cluster)
N_clustered <- N_data %>% mutate(cluster=assignment$cluster)
N_clustered <- N_clustered %>% group_by(cluster) %>% summarise_all(list(sum))
N_clustered <- as.data.frame(N_clustered %>% select(-cluster))
R_hat <- N_clustered / rowSums(N_clustered)
log_R_hat <- matrix(0,nrow(R_hat),d,byrow=TRUE)
for(i in 1:nrow(R_hat))
{
for(l in 1:d)
{
if(R_hat[i,l]==0)
{
log_R_hat[i,l]=0
}else{
log_R_hat[i,l] = log(R_hat[i,l])
}
}
}
BIC[j] = as.numeric(-2*sum(as.matrix(N_clustered)*log_R_hat) + (nrow(R_hat))*(d-1)*log(n))
print(j)
}
lambda_min <- lambda_seq[which.min(BIC)]
#plot(lambda_seq,BIC,type="l")
A <- create_adjacency(model_all$V[[which.min(BIC)]],w,ncol(X))
opt_assignment <- find_clusters(A)
unique_cluster_id <- unique(opt_assignment$cluster)
opt_assignment
N_clustered <- N_data %>% mutate(cluster=opt_assignment$cluster)
N_clustered <- N_clustered %>% group_by(cluster) %>% summarise_all(list(sum))
N_clustered_counts <- as.data.frame(N_clustered %>% select(-cluster))
R_hat <- N_clustered_counts / rowSums(N_clustered_counts)
stationary_prob <- rowSums(N)/sum(N)
final_model <- list("cluster"=opt_assignment$cluster,"size"=opt_assignment$size,
"state space"=all_comb,"transition matrix"=R_hat,"stationary prob"=stationary_prob,
"lambda"=lambda_min)
final_model
}
setwd("C:/Users/tmajumd/Desktop/Graduate Studies/PhD/Data Analysis/Gene Classification SMM/Reference  Genes")
dat_sars <- read.fasta(file="/SARS_Cov2_Ref.fasta")
setwd("C:/Users/tmajumd/Desktop/Graduate Studies/PhD/Data Analysis/Gene Classification SMM/Reference  Genes")
dat_sars <- read.fasta(file="SARS_Cov2_Ref.fasta")
